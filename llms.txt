# REST API Downloader - LLM Documentation

> Documentation optimized for Large Language Models (LLMs) and AI assistants
> Last updated: October 2025

## Overview

REST API Downloader is a production-ready FastAPI service for downloading and converting web content.
It provides synchronous single-URL downloads and asynchronous batch processing with multiple output formats.

**Base URL**: `http://localhost:8000` (default)
**Tech Stack**: Python 3.10+, FastAPI, httpx, BeautifulSoup, Playwright, Redis (optional)
**Status**: Production Ready (v0.0.1)

## Quick Start for LLMs

### Making a Simple Request

```python
import httpx

# Extract article text from a URL (default format)
response = httpx.get("http://localhost:8000/https://example.com")
text_content = response.text

# Get markdown format
response = httpx.get(
    "http://localhost:8000/https://example.com",
    headers={"Accept": "text/markdown"}
)
markdown_content = response.text

# Generate PDF
response = httpx.get(
    "http://localhost:8000/https://example.com",
    headers={"Accept": "application/pdf"}
)
pdf_bytes = response.content
```

### Authentication (if enabled)

```python
# Using Bearer token
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Accept": "text/plain"
}

# OR using X-API-Key header
headers = {
    "X-API-Key": "YOUR_API_KEY",
    "Accept": "text/plain"
}

response = httpx.get("http://localhost:8000/https://example.com", headers=headers)
```

## API Endpoints Reference

### 1. Health Check
```http
GET /health
```
Returns service status, version, and configuration info.

**Response**:
```json
{
  "status": "healthy",
  "version": "0.0.1",
  "auth_enabled": false,
  "redis_available": true,
  "rate_limiting_enabled": true
}
```

### 2. Single URL Download
```http
GET /{url}
Accept: {format}
```

**URL Encoding**: The URL must be properly encoded but NOT double-encoded
- ✅ Correct: `GET /https://example.com/page?id=123`
- ❌ Wrong: `GET /https%3A%2F%2Fexample.com%2Fpage%3Fid%3D123`

**Supported Accept Headers**:
| Accept Header | Output | Use Case |
|--------------|--------|----------|
| `text/plain` | Plain text | Article content, clean text extraction |
| `text/markdown` | Markdown | Structured content with headings/links |
| `text/html` | HTML | Original HTML source |
| `application/json` | JSON | Metadata + base64 content |
| `application/pdf` | PDF | JavaScript-rendered PDF (Playwright) |
| (none) | **Default: text/plain** | Clean article text |

**Response Headers**:
- `Content-Type`: Format of the response
- `X-Original-URL`: The actual URL that was fetched (after redirects)
- `X-Content-Length`: Size of original content
- `X-RateLimit-Limit`: Rate limit for endpoint
- `X-RateLimit-Remaining`: Remaining requests
- `X-RateLimit-Reset`: Unix timestamp of reset

**Example**:
```python
response = httpx.get(
    "http://localhost:8000/https://news.ycombinator.com",
    headers={"Accept": "text/plain"}
)
print(response.text)  # Clean article text
```

### 3. Batch Processing (Async Jobs)

#### Submit Batch Job
```http
POST /batch
Content-Type: application/json
```

**Request Body**:
```json
{
  "urls": [
    {"url": "https://example.com", "format": "text"},
    {"url": "https://github.com", "format": "markdown"},
    {"url": "https://news.ycombinator.com"}
  ],
  "default_format": "text",
  "concurrency_limit": 10,
  "timeout_per_url": 30
}
```

**Parameters**:
- `urls` (required): Array of URL objects (max 50)
  - `url` (required): URL to download
  - `format` (optional): Format for this URL (text/markdown/html/pdf/json/raw)
- `default_format` (optional): Format for URLs without specified format (default: "text")
- `concurrency_limit` (optional): Max parallel requests (default: 10, max: 50)
- `timeout_per_url` (optional): Timeout per URL in seconds (default: 30)

**Response**:
```json
{
  "job_id": "job_abc123",
  "status": "pending",
  "created_at": "2025-10-04T12:00:00Z",
  "total_urls": 3,
  "estimated_completion": "2025-10-04T12:00:06Z"
}
```

#### Check Job Status
```http
GET /jobs/{job_id}/status
```

**Response**:
```json
{
  "job_id": "job_abc123",
  "status": "completed",
  "progress": 100,
  "created_at": "2025-10-04T12:00:00Z",
  "started_at": "2025-10-04T12:00:01Z",
  "completed_at": "2025-10-04T12:00:05Z",
  "total_urls": 3,
  "processed_urls": 3,
  "successful_urls": 3,
  "failed_urls": 0,
  "results_available": true,
  "expires_at": "2025-10-11T12:00:05Z"
}
```

**Job Statuses**:
- `pending`: Job queued, not started
- `running`: Job in progress
- `completed`: Job finished successfully
- `failed`: Job failed with error
- `cancelled`: Job was cancelled

#### Download Results
```http
GET /jobs/{job_id}/results
```

**Response**:
```json
{
  "job_id": "job_abc123",
  "status": "completed",
  "created_at": "2025-10-04T12:00:00Z",
  "completed_at": "2025-10-04T12:00:05Z",
  "total_duration": 4.23,
  "results": [
    {
      "url": "https://example.com",
      "success": true,
      "format": "text",
      "content": "Example Domain...",
      "size": 1256,
      "content_type": "text/html",
      "duration": 1.42,
      "status_code": 200
    },
    {
      "url": "https://github.com",
      "success": true,
      "format": "markdown",
      "content": "# GitHub\n\n...",
      "size": 45678,
      "content_type": "text/html",
      "duration": 2.15,
      "status_code": 200
    }
  ],
  "summary": {
    "total_requests": 3,
    "successful_requests": 3,
    "failed_requests": 0,
    "success_rate": 100.0,
    "total_duration": 4.23
  }
}
```

**Note**: PDF and raw formats use `content_base64` field instead of `content`

#### Cancel Job
```http
DELETE /jobs/{job_id}
```

**Response**:
```json
{
  "success": true,
  "message": "Job job_abc123 cancelled successfully"
}
```

## Error Handling

All errors return JSON with consistent structure:

```json
{
  "detail": {
    "error": "Human-readable error message",
    "error_type": "validation_error",
    "status_code": 400
  }
}
```

**Common Error Types**:
- `validation_error` (400): Invalid URL or request format
- `authentication_error` (401): Missing or invalid API key
- `timeout_error` (408): Request exceeded timeout
- `rate_limit_exceeded` (429): Too many requests
- `http_error` (502): Upstream HTTP error
- `pdf_generation_error` (500): PDF generation failed
- `internal_error` (500): Server error
- `service_unavailable` (503): Service temporarily unavailable (e.g., Redis down)

**HTTP Status Codes**:
- `200`: Success
- `400`: Bad request (invalid URL, validation error)
- `401`: Unauthorized (missing/invalid API key)
- `404`: Not found (job ID doesn't exist)
- `408`: Timeout
- `429`: Rate limit exceeded
- `500`: Internal server error
- `502`: Bad gateway (upstream error)
- `503`: Service unavailable

## Rate Limiting

**Default Limits** (configurable via environment):
- Download endpoints (`GET /{url}`): **60 requests/minute**
- Batch endpoints (`POST /batch`): **20 requests/minute**
- Status endpoints (`GET /health`, `/metrics`): **200 requests/minute**
- Other endpoints: **100 requests/minute**

**Rate Limit Headers** (included in all responses):
```http
X-RateLimit-Limit: 60
X-RateLimit-Remaining: 59
X-RateLimit-Reset: 1696425600
```

**When Rate Limited**:
```http
HTTP/1.1 429 Too Many Requests
Retry-After: 42

{
  "detail": "Rate limit exceeded: 60 per 1 minute"
}
```

## Configuration

### Environment Variables

**Authentication**:
```bash
DOWNLOADER_KEY=your-secret-key  # Optional API key
```

**Redis (for batch jobs and distributed rate limiting)**:
```bash
REDIS_URI=redis://localhost:6379  # Optional, enables batch processing
```

**Rate Limiting**:
```bash
RATELIMIT_ENABLED=true                    # Enable/disable (default: true)
RATELIMIT_DEFAULT_LIMIT=100/minute        # Default limit
RATELIMIT_DOWNLOAD_LIMIT=60/minute        # Download endpoint limit
RATELIMIT_BATCH_LIMIT=20/minute           # Batch endpoint limit
RATELIMIT_STORAGE_URI=redis://localhost:6379  # Optional, uses REDIS_URI if not set
```

**Performance Tuning**:
```bash
# HTTP Client
HTTP_MAX_CONNECTIONS=200         # Max total connections (default: 200)
HTTP_REQUEST_TIMEOUT=30          # Request timeout seconds (default: 30)

# Batch Processing
BATCH_CONCURRENCY=40            # Max concurrent batch requests (default: CPU×8, max 50)
BATCH_MAX_URLS_PER_BATCH=50     # Max URLs per batch (default: 50)

# PDF Generation
PDF_CONCURRENCY=8               # Max concurrent PDFs (default: CPU×2, max 12)
PDF_PAGE_LOAD_TIMEOUT=30000     # Playwright timeout ms (default: 30000)

# Content
CONTENT_MAX_DOWNLOAD_SIZE=52428800  # Max download size bytes (default: 50MB)
```

**Security**:
```bash
SSRF_BLOCK_PRIVATE_IPS=true      # Block private IPs (default: true)
SSRF_BLOCK_CLOUD_METADATA=true   # Block cloud metadata endpoints (default: true)
CORS_ALLOWED_ORIGINS=*           # Comma-separated origins (default: *)
```

## Example Workflows for LLMs

### Workflow 1: Extract Article Content
```python
import httpx

async def extract_article(url: str) -> str:
    """Extract clean article text from any URL."""
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f"http://localhost:8000/{url}",
            headers={"Accept": "text/plain"}
        )
        response.raise_for_status()
        return response.text

# Usage
article = await extract_article("https://news.ycombinator.com")
print(article)
```

### Workflow 2: Batch Process Multiple URLs
```python
import httpx
import asyncio

async def batch_download_urls(urls: list[str]) -> dict:
    """Download multiple URLs asynchronously with batch processing."""
    async with httpx.AsyncClient(timeout=60.0) as client:
        # Submit batch job
        batch_request = {
            "urls": [{"url": url, "format": "markdown"} for url in urls],
            "default_format": "markdown",
            "concurrency_limit": 10
        }

        response = await client.post(
            "http://localhost:8000/batch",
            json=batch_request
        )
        job = response.json()
        job_id = job["job_id"]

        # Poll for completion
        while True:
            status_response = await client.get(
                f"http://localhost:8000/jobs/{job_id}/status"
            )
            status = status_response.json()

            if status["status"] == "completed":
                break
            elif status["status"] == "failed":
                raise Exception(f"Job failed: {status.get('error_message')}")

            await asyncio.sleep(2)  # Poll every 2 seconds

        # Download results
        results_response = await client.get(
            f"http://localhost:8000/jobs/{job_id}/results"
        )
        return results_response.json()

# Usage
urls = [
    "https://example.com",
    "https://github.com",
    "https://news.ycombinator.com"
]
results = await batch_download_urls(urls)
for result in results["results"]:
    print(f"{result['url']}: {result['success']}")
```

### Workflow 3: Generate PDF with Error Handling
```python
import httpx
from pathlib import Path

async def download_as_pdf(url: str, output_path: Path) -> bool:
    """Download URL as PDF with comprehensive error handling."""
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            response = await client.get(
                f"http://localhost:8000/{url}",
                headers={"Accept": "application/pdf"}
            )

            if response.status_code == 200:
                output_path.write_bytes(response.content)
                return True
            elif response.status_code == 429:
                # Rate limited
                retry_after = response.headers.get("Retry-After", 60)
                print(f"Rate limited. Retry after {retry_after}s")
                return False
            elif response.status_code == 503:
                # Service temporarily unavailable (PDF queue full)
                print("PDF service busy. Try again later.")
                return False
            else:
                error = response.json()
                print(f"Error: {error['detail']['error']}")
                return False

        except httpx.TimeoutException:
            print("Request timeout")
            return False
        except Exception as e:
            print(f"Unexpected error: {e}")
            return False

# Usage
success = await download_as_pdf(
    "https://example.com",
    Path("example.pdf")
)
```

### Workflow 4: Check Service Health
```python
import httpx

async def check_service_health() -> dict:
    """Check if service is healthy and what features are available."""
    async with httpx.AsyncClient() as client:
        response = await client.get("http://localhost:8000/health")
        health = response.json()

        return {
            "is_healthy": health["status"] == "healthy",
            "auth_required": health.get("auth_enabled", False),
            "batch_available": health.get("redis_available", False),
            "rate_limiting": health.get("rate_limiting_enabled", True),
            "version": health.get("version", "unknown")
        }

# Usage
health = await check_service_health()
if health["batch_available"]:
    print("Batch processing is available")
else:
    print("Batch processing requires Redis")
```

## Important Notes for LLMs

### Content Extraction
- **Default format is text/plain**: Returns clean article text, not raw HTML
- **Automatic JavaScript rendering**: If initial extraction yields little content, service automatically falls back to Playwright for JS-heavy sites
- **Smart extraction**: Uses BeautifulSoup to target `<article>`, `<main>`, and content areas
- **Metadata included**: Check response headers for original URL, content length, etc.

### PDF Generation
- **Resource intensive**: Limited concurrency (default: CPU×2, max 12)
- **May return 503**: If PDF queue is full, retry after a delay
- **Uses Playwright**: Full browser rendering with JavaScript support
- **Timeout**: Default 30s page load timeout (configurable)

### Batch Processing
- **Requires Redis**: If REDIS_URI not set, batch endpoints return 503
- **Async execution**: Jobs run in background, poll status for completion
- **Results expire**: Default 7 days (configurable)
- **Max 50 URLs**: Per batch request (configurable)
- **Partial success**: Batch completes even if some URLs fail

### Rate Limiting
- **Enabled by default**: Can be disabled with `RATELIMIT_ENABLED=false`
- **Per-endpoint limits**: Different limits for different endpoint types
- **Redis for distributed**: Without Redis, uses in-memory (single instance only)
- **Headers included**: Always check X-RateLimit-* headers

### Security
- **SSRF protection**: Blocks private IPs and cloud metadata by default
- **URL validation**: Comprehensive validation before making requests
- **Optional auth**: Set DOWNLOADER_KEY to require authentication
- **No secrets in URLs**: Never include API keys in downloaded URLs

### Error Handling Best Practices
1. Always check HTTP status code
2. Parse error JSON for detailed error_type and message
3. Handle 429 (rate limit) with exponential backoff
4. Handle 503 (service unavailable) by retrying
5. Handle timeouts gracefully (408)

## Running the Service

### Docker (Recommended)
```bash
# Build
docker build -t downloader .

# Run without auth
docker run -p 8000:8000 downloader

# Run with auth and Redis
docker run \
  -e DOWNLOADER_KEY=your-secret-key \
  -e REDIS_URI=redis://redis:6379 \
  -p 8000:8000 \
  downloader
```

### Local Development
```bash
# Install dependencies
uv sync

# Install Playwright browsers (for PDF)
uv run playwright install chromium

# Run server
uv run python run.py

# Run tests
uv run pytest -m smoke  # Quick smoke tests (<3s)
uv run pytest           # All tests
```

## Architecture Overview

**Components**:
- **FastAPI**: Async web framework
- **httpx**: HTTP client with connection pooling
- **BeautifulSoup**: HTML parsing and article extraction
- **Playwright**: Headless browser for PDF generation and JS rendering
- **Redis**: Optional, for batch jobs and distributed rate limiting
- **slowapi**: Rate limiting middleware

**Request Flow**:
1. Request received → Rate limit check → Authentication check (if enabled)
2. URL validation → SSRF protection check
3. Content download (httpx) → Content conversion (BeautifulSoup)
4. If minimal content → Playwright fallback (for JS-heavy sites)
5. Response formatted per Accept header → Response sent

**Concurrency Limits** (defaults):
- HTTP connections: 200 max, 100 keepalive
- Batch processing: CPU×8 concurrent requests (max 50)
- PDF generation: CPU×2 concurrent browsers (max 12)

## Testing

**Test Tiers**:
- **Smoke** (80 tests, <3s): Fast sanity checks
- **Integration** (~15s): Component integration tests
- **E2E** (~60s): Full end-to-end workflows

```bash
# Run smoke tests (fastest)
uv run pytest -m smoke -v

# Run integration tests
uv run pytest -m integration -v

# Run E2E tests (slowest)
uv run pytest -m e2e -v

# Run all tests
uv run pytest
```

## Common Pitfalls

1. **Double URL encoding**: Don't encode the URL parameter twice
   - ✅ `GET /https://example.com?id=123`
   - ❌ `GET /https%3A%2F%2Fexample.com%3Fid%3D123`

2. **Missing Accept header**: Defaults to text/plain (article extraction)
   - Use `Accept: text/html` for raw HTML
   - Use `Accept: application/json` for structured data

3. **Batch without Redis**: Batch endpoints require Redis
   - Check `/health` for `redis_available: true`
   - Set `REDIS_URI` environment variable

4. **PDF timeouts**: PDF generation can be slow
   - Increase timeout on client side (60s recommended)
   - Check for 503 response (service busy)
   - Use batch processing for multiple PDFs

5. **Rate limiting**: Don't ignore rate limit headers
   - Check `X-RateLimit-Remaining` before making requests
   - Implement backoff when `X-RateLimit-Remaining: 0`
   - Wait for `X-RateLimit-Reset` timestamp

6. **Private IPs**: SSRF protection blocks private IPs by default
   - For testing: Set `SSRF_BLOCK_PRIVATE_IPS=false`
   - For production: Keep enabled

## Support & Resources

- **Documentation**: `/doc` directory in repository
- **Examples**: `/examples` directory with Python examples
- **Product Docs**: `/product` directory with PRD, roadmap, architecture
- **Configuration**: See `src/downloader/config.py` for all settings
- **API Reference**: `/doc/api-reference.md`

## Version History

- **v0.0.1** (October 2025): Production-ready release
  - ✅ Single URL and batch processing
  - ✅ 5 output formats (text, markdown, HTML, JSON, PDF)
  - ✅ Rate limiting with DoS protection
  - ✅ 248 tests (3-tier strategy)
  - ✅ SSRF protection and security
  - ✅ Optional authentication
  - ✅ Comprehensive configuration (40+ settings)

---

**End of LLM Documentation**

For human-readable docs, see README.md and /doc directory.
For product strategy, see /product directory.
For code examples, see /examples directory.
